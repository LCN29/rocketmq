ServiceThread
    - AcceptSocketService in HAService
    - AclFileWatchService
    - AllocateMappedFileService
    - CommitRealTimeService in CommitLog
    - FileWatchService
    - FlushCommitLogService in CommitLog
    - FlushConsumeQueueService in DefaultMessageStore
    - FlushDiskWatcher
    - FlushRealTimeService in CommitLog
    - GroupCommitService in CommitLog
    - GroupTransferService in HAService
    - HAClient in HAService
    - LmqPullRequestHoldService
    - PullMessageService
    - PullRequestHoldService
    - ReadSocketService in HAConnection
    - RebalanceService
    - ReputMessageService in DefaultMessageStore
    - StoreStatsService
    - TransactionalMessageCheckService
    - WriteSocketService in HAConnection



## RebalanceService 消费端 - 消费队列负载均衡

遍历 MQClientInstance 的 ConcurrentMap<String, MQConsumerInner> consumerTable (当前客户端中维护的 消费组对应的客户端 DefaultMQPushConsumerImpl)

然后调用 RebalanceImpl 的 doRebalance 方法进行再平衡

```log
1. 获取当前维护消费端的订阅消息 (ConcurrentMap<String, SubscriptionData> subscriptionInner, Topic 名称和对应的订阅消息)
2. 进行遍历, 对每个 Topic 进行再平衡
3. 获取当前 Topic 的消息队列 (ConcurrentMap<String, Set<MessageQueue>> topicSubscribeInfoTable, 消息队列和对应的处理队列)
4. 获取当前 Topic 的所有消费者 Id (向 Broker 发送了 GET_CONSUMER_LIST_BY_GROUP = 38 请求码， cid 格式: IP@端口#1887994150767625， 暂时不知道是什么， 查看 org.apache.rocketmq.client.ClientConfig#buildMQClientId)
5. 对所有的队列和消费者 id 进行排序
6. 调用自身维护的分配策略进行分配 AllocateMessageQueueStrategy allocate 方法分配，当前消费端对应的消费队列 (入参有个 cid, 调用时传入当前消费端的消费者 id)
7. 遍历当前消费者维护的处理的队列关系 (ConcurrentMap<MessageQueue, ProcessQueue> processQueueTable, 消息队列和队列处理信息)
    1. 已有的队列关系中的队列在新分配到的队列列表中 (这个队列还是由当前消费者处理), 跳过这个队列
    2. 向 Broker 更新当前消费者的对应队列的消费进度 (向 Broker 发送了 UPDATE_CONSUMER_OFFSET = 15 请求码)
    3. 清除当前消费者的消费进度, 从 Map 中删除这个队列的信息 (RemoteBrokerOffsetStore 的 ConcurrentMap<MessageQueue, AtomicLong> offsetTable)
    4. Push 模式下且当前消费者是有序消费，且是集群消费，那么尝试从Broker端将该消息队列解锁，如果是并发消费，则不会解锁
8. 遍历分配到的队列 
    1. 对应的队列再重新分配前就有处理, 跳过这个队列
    2. 尝试将当前队列的消费进度从当前消费端进行删除 (RemoteBrokerOffsetStore 的 ConcurrentMap<MessageQueue, AtomicLong> offsetTable)
    3. 向 Broker 查询对应队列的消费进度 (向 Broker 发送了 QUERY_CONSUMER_OFFSET = 14 请求码)
    4. 将查询到的消费进度设置到 OffsetStore 中 (RemoteBrokerOffsetStore 的 ConcurrentMap<MessageQueue, AtomicLong> offsetTable)
    5. 向 前消费者处理的队列关系 (ConcurrentMap<MessageQueue, ProcessQueue> processQueueTable, 消息队列和队列处理信息) 添加当前的队列信息
    6. 向 拉取消息线程 PullMessageService 追加一个拉取消息的请求
9. 向 Broker 发送心跳信息 (向 Broker 发送了 HEART_BEAT = 34 请求码), 消息中会包含当前消费端的订阅信息
```

* Broker 在下面的场景
> 1 Broker 收到心跳请求之后如果发现消息中有新的 consumer 连接或者 consumer 订阅了新的 topic 或者移除了 topic 的订阅
> 2 如果某个客户端连接出现连接异常事件 EXCEPTION、连接断开事件 CLOSE、或者连接闲置事件 IDLE

向客户端发送请求码 NOTIFY_CONSUMER_IDS_CHANGED = 40, 通知客户端需要再平衡
客户端收到后, 触发 org.apache.rocketmq.client.impl.ClientRemotingProcessor#notifyConsumerIdsChanged 方法, 立即唤醒 RebalanceService 的线程, 进行再平衡

* 消费端启动时, 也会立即执行一次 RebalanceImpl 的 doRebalance, 通过再平衡, 分配消费队列
* RebalanceService 自身会 20s 唤醒一次, 进行再平衡

## PullMessageService 消费端 - 拉取消息

在再平衡中, 会向 PullMessageService 的队列 (LinkedBlockingQueue<PullRequest> pullRequestQueue) 添加一个拉取消息的请求 PullRequest
在 Push 模式下, 这些请求会被 PullMessageService 的线程消费

PullMessageService 的线程会从 pullRequestQueue 中取出 PullRequest, 然后执行 org.apache.rocketmq.client.impl.consumer.PullMessageService#pullMessage 方法

```log
1. 从 PullRequest 中获取对应的消费者组 (consumerGroup)
2. 从 MQClientInstance 的 ConcurrentMap<String, MQConsumerInner> consumerTable 中获取对应的消费者 DefaultMQPushConsumerImpl
3. 从 PullRequest 中获取对应的消息处理队列 (ProcessQueue)
4. ProcessQueue 的 dropped 为 true, 说明当前队列已经被丢弃, 跳过这个队列的拉取请求
5. 更新这个拉取请求的最后拉取时间 lastPullTimestamp 为当前的时间
6. 判断当前的消费者客户端的状态为 RUNNING, 否则跳过这个队列的拉取请求
7. 如果当前的消费者客户端的状态为 pause (再平衡中), 则跳过这个队列的拉取请求
8. 从 ProcessQueue 获取当前的缓存的消息的条数/消息大小, 如果超过了配置的最大值, 则在 50 毫秒后再将这个拉取请求添加到 PullMessageService 的队列中, 跳过这个队列的拉取请求
9. 如果当前的消费客户端是不是有序消费, 进行不同的属性赋值
    - 如果不是有序消费
    1. 从 ProcessQueue 的 TreeMap<Long, MessageExt> msgTreeMap 中获取第一个消息和最后一个消息的位点差, 如果超过了配置的位点差 (2000), 则在 50 毫秒后再将这个拉取请求添加到 PullMessageService 的队列中, 跳过这个队列的拉取请求

    - 如果是有序消费
    1. 从 ProcessQueue 的 locked 判断当前队列是否锁住了, 没有，则在 3000 毫秒后再将这个拉取请求添加到 PullMessageService 的队列中, 跳过这个队列的拉取请求
    2. 向 Broker 查询对应队列的消费进度 (向 Broker 发送了 QUERY_CONSUMER_OFFSET = 14 请求码)
    3. 更新 PullRequest 的 previouslyLocked 为 true (?)
    4. 更新 ProcessQueue 的下次请求到的位点 nextOffset 为从 Broker 请求到的消费进度
10. 从 RebalanceImpl 的 ConcurrentMap<String, SubscriptionData> subscriptionInner 获取 Topic 对应的订阅消息
11. 创建出一个回调函数 PullCallback, 在回调函数中处理拉取到的消息 (这个后面分析)
12. 如果是集群模式, 从 OffsetStore 中获取当前队列的消费进度, 如果这个值大于 0, 表示可以上报消费位点给 Broker, commitOffsetEnable 变为 true
13. 从 RebalanceImpl 的 ConcurrentMap<String, SubscriptionData> subscriptionInner 获取  Topic 对应的订阅消息
14. 调用 org.apache.rocketmq.client.impl.consumer.PullAPIWrapper#pullKernelImpl 方法进行拉取消息
15. 向 Broker 发送拉取消息的请求 (向 Broker 发送了 PULL_MESSAGE = 11 请求码), 发送的消息中有个 sysFlag 标示 (二进制的值，第二位标示请求是否可以挂起, 这里给的是 true)
16. 后面收到 Broker 发送的消息后, 会调用 PullCallback 的方法进行处理


// 收到消息后, 成功的处理逻辑
1. 更新 PullAPIWrapper 的 ConcurrentMap<MessageQueue, AtomicLong> pullFromWhichNodeTable (记录当前队列从哪个 Broker 拉取的消息)
2. 判断响应结果 PullResult 的状态
    - NO_NEW_MSG / NO_MATCHED_MSG(没有新消息 / 没有匹配的消息)
    1. 更新拉取请求 PullRequest 的下次请求位点 为 响应结果的 nextBeginOffset
    2. 对应的处理队列 ProcessQueue 的 msgCount 为 0 (当前队列没有消息), 更新 OffsetStore 的对应队列的消费进度为 nextBeginOffset
    3. 重新将这个拉取请求添加到 PullMessageService 的队列中

    - OFFSET_ILLEGAL 消费位点非法
    1. 更新拉取请求 PullRequest 的下次请求位点 为 响应结果的 nextBeginOffset
    2. 更新对应的处理队列 ProcessQueue 为丢弃状态 dropped = true
    3. 提交一个 10s 后执行的任务到线程池, 执行下面的逻辑
        3.1 更新设置到 OffsetStore 中 (RemoteBrokerOffsetStore 的 ConcurrentMap<MessageQueue, AtomicLong> offsetTable) 中对应队列的消费进度为 nextBeginOffset
        3.2 向 Broker 更新当前消费者的对应队列消费进度 (向 Broker 发送了 UPDATE_CONSUMER_OFFSET = 15 请求码)
        3.3 将这个队列和处理队列从 RebalanceImpl 的 ConcurrentMap<MessageQueue, ProcessQueue> processQueueTable 中删除
        3.4 向 Broker 更新当前消费者的对应队列消费进度 (向 Broker 发送了 UPDATE_CONSUMER_OFFSET = 15 请求码)
        3.5 清除当前消费者的消费进度, 从 Map 中删除这个队列的信息 (RemoteBrokerOffsetStore 的 ConcurrentMap<MessageQueue, AtomicLong> offsetTable)
        3.6 Push 模式下且当前消费者是有序消费，且是集群消费，那么尝试从Broker端将该消息队列解锁，如果是并发消费，则不会解锁

    - FOUND 找到消息
    1. 从 RebalanceImpl 的 ConcurrentMap<String, SubscriptionData> subscriptionInner 获取  Topic 对应的订阅消息
    2. 从 SubscriptionData 获取对应订阅的 Tags, 过滤掉获取到的消息的 Tags 不在需要的 Tags 中的消息
    3. 遍历剩余的消息的 TRAN_MSG 属性 (是否为事务消息), 如果为 true, 将消息的 UNIQ_KEY 属性设置到消息的 transactionId(事务 id) 属性
    4. 遍历剩余的消息的, 给消息 MIN_OFFSET 属性设置为当前消息的 minOffset 属性, 给消息 MAX_OFFSET 属性设置为当前消息的 maxOffset 属性, 设置消息的 BrokerName 属性为 MessageQueue 的 BrokerName 属性
    5. 更新拉取请求 PullRequest 的下次请求位点 为 响应结果的 nextBeginOffset
    6. 如果响应的消息为空, 重新将这个拉取请求添加到 PullMessageService 的队列中, 结束
    7. 将所有的消息保存到 处理队列 ProcessQueue 的 TreeMap<Long, MessageExt> msgTreeMap 中, key 为对应消息的消费位点
    8. 根据消息, 更新 ProcessQueue 的 msgCount 和 msgSize
    9. 获取最后一个消息的 MAX_OFFSET 属性, 获取到时, 这个值减去当前消息的消费位点， 如果大于 0, 赋值给 ProccessQueue 的 msgAccCnt (未知)
    10. 将消息提交给 org.apache.rocketmq.client.impl.consumer.ConsumeMessageService#submitConsumeRequest (并发消费/顺序消费)
    11. 重新将这个拉取请求添加到 PullMessageService 的队列中
    12. 根据配置的延迟间隔, 延迟提交这个拉取消息的 PullRequest 到 PullMessageService 的队列中 (默认 0, 立即拉取)


org.apache.rocketmq.client.impl.consumer.ConsumeMessageService#submitConsumeRequest 消费消息

并发消费 ConsumeMessageConcurrentlyService
1. 获取消费者每次批量消费时，最多消费多少条消息
2. 获取到的消息个数小于配置的最大消费消息数, 将消息 List<MessageExt> + 消息队列 MessageQueue + 处理队列 ProcessQueue 封装为 org.apache.rocketmq.client.impl.consumer.ConsumeMessageConcurrentlyService.ConsumeRequest, 提交到线程池中
3. 按照配置的最大消费消息数, 将消息 List<MessageExt> 拆分为多个 ConsumeRequest, 提交到线程池中

顺序消费 ConsumeMessageOrderlyService
1. 将消息 消息队列 MessageQueue + 处理队列 ProcessQueue 封装为 org.apache.rocketmq.client.impl.consumer.ConsumeMessageOrderlyService.ConsumeRequest, 提交到线程池中 (注 2 个不同的 ConsumeRequest， 里面消费逻辑不一样)


org.apache.rocketmq.client.impl.consumer.ConsumeMessageConcurrentlyService.ConsumeRequest 的 run 方法
1. 获取处理队列 ProcessQueue 的丢弃属性 dropped, 如果为 true, 跳过这个消息的消费
2. 遍历所有的消息, 如果消息有 RETRY_TOPIC 属性, 设置他们的 topic 为这个属性值
3. 遍历所有的消息, 如果当前消费端有设置命名空间, 将命名空间加到消息的 topic 上，将最新的值重新设置为消息的 topic
4. 当前的消费客户端有钩子函数，则执行钩子函数
5. 遍历所有的消息, 给消息的 CONSUME_START_TIME 属性设置为当前时间戳
6. 调用注册在 DefaultMQPushConsumerImpl 的 MessageListener 的 consumeMessage 方法进行消费, 获取执行结果
7. 当前的消费客户端有钩子函数，则执行钩子函数
8. 处理队列 ProcessQueue 的 的丢弃属性 dropped, 如果为 true, 跳过
9. 当前的消费端的消费模式为集群模式
    - 遍历所有消息, 向 Broker 更新消息消费响应 (向 Broker 发送了 CONSUMER_SEND_MSG_BACK = 36 请求码)
    - 向 Broker 更新失败的消息保存下来，延迟 5000ms 后, 重新将这批失败的消息 + 消息队列 MessageQueue + 处理队列 ProcessQueue 封装为 ConsumeRequest, 提交到线程池中
    - 获取最新的消费进度, 如果提交的消费位点大于 0, 同时处理队列未丢弃, 向 Broker 更新当前消费者的对应队列的消费进度 (向 Broker 发送了 UPDATE_CONSUMER_OFFSET = 15 请求码)




org.apache.rocketmq.client.impl.consumer.ConsumeMessageOrderlyService.ConsumeRequest 的 run 方法

1. 获取处理队列 ProcessQueue 的丢弃属性 dropped, 如果为 true, 跳过这个消息的消费
2. 获取这个队列的锁, 获取到才执行
3. 当前消费客户端为广播模式 || 处理队列 ProcessQueue 的锁住状态 locked 为 true 并且锁住没有超时
    - 符合条件
    1. 再次获取处理队列 ProcessQueue 的丢弃属性 dropped, 如果为 true, 跳过这个消息的消费
    2. 如果是集群模式同时 ProcessQueue 的锁住状态 locked 为 false，延迟多少毫秒后, 重新将消息队列 MessageQueue + 处理队列 ProcessQueue 封装为 ConsumeRequest, 提交到线程池中 (重新获取队列的锁成功， 延迟 110ms， 获取锁失败， 3100ms)
    3. 如果是集群模式同时 ProcessQueue 的锁住状态 locked 为 true, 但是锁超时了, 延迟多少毫秒后, 重新将消息队列 MessageQueue + 处理队列 ProcessQueue 封装为 ConsumeRequest, 提交到线程池中 (重新获取队列的锁成功， 延迟 110ms， 获取锁失败， 3100ms)
    4. 进入锁的时间超过了 60000ms, 延迟多少毫秒后, 重新将消息队列 MessageQueue + 处理队列 ProcessQueue 封装为 ConsumeRequest, 提交到线程池中 (重新获取队列的锁成功， 延迟 110ms， 获取锁失败， 3100ms)
    5. 消费者每次批量消费时，最多消费多少条消息, 从 ProcessQueue 的 TreeMap<Long, MessageExt> msgTreeMap 中获取对应条数的消息
    6. 遍历所有的消息, 如果消息有 RETRY_TOPIC 属性, 设置他们的 topic 为这个属性值
    7. 遍历所有的消息, 如果当前消费端有设置命名空间, 将命名空间加到消息的 topic 上，将最新的值重新设置为消息的 topic
    8. 当前的消费客户端有钩子函数，则执行钩子函数
    9. 获取消费队列 ProcessQueue 的消费锁 Lock consumeLock
    10. 调用注册在 DefaultMQPushConsumerImpl 的 MessageListener 的 consumeMessage 方法进行消费, 获取执行结果, 然后释放锁
    11. 如果消息是自动提交
        - 自动提交
        1. 执行结果为 暂时挂起
            1.1 判断所以需要处理的消息的重试次数是否超过了配置的最大重试次数 (默认为 Interger.MAX_VALUE), 超过了, 消息默认提交, 即执行完成 （更新处理队列的 ProcessQueue 的 msgSize 和 msgCount）
            1.2 没有超过, 延迟 3000ms 后, 重新将消息队列 MessageQueue + 处理队列 ProcessQueue 封装为 ConsumeRequest, 提交到线程池中
        2. 执行结果为 提交/回滚/消费成功
            2.1 消息提交, 即执行完成 （更新处理队列的 ProcessQueue 的 msgSize 和 msgCount）
        - 非自动提交
            1.1 状态为提交, 进行消息提交, 即执行完成 （更新处理队列的 ProcessQueue 的 msgSize 和 msgCount）
            1.2 状态为回滚, 重新将消费的消息提交到处理队列 ProcessQueue 的 TreeMap<Long, MessageExt> msgTreeMap 中, 延迟 3000ms 后, 重新将消息队列 MessageQueue + 处理队列 ProcessQueue 封装为 ConsumeRequest, 提交到线程池中
            1.3 转提为暂时挂起, 延迟 3000ms 后, 重新将消息队列 MessageQueue + 处理队列 ProcessQueue 封装为 ConsumeRequest, 提交到线程池中
    12. 如果提交的消费位点大于 0, 同时处理队列未丢弃, 向 Broker 更新当前消费者的对应队列的消费进度 (向 Broker 发送了 UPDATE_CONSUMER_OFFSET = 15 请求码)        

    - 不符合条件
    1. 再次获取处理队列 ProcessQueue 的丢弃属性 dropped, 如果为 true, 跳过这个消息的消费
    2. 延迟多少毫秒后, 重新将消息队列 MessageQueue + 处理队列 ProcessQueue 封装为 ConsumeRequest, 提交到线程池中 (重新获取队列的锁成功， 延迟 110ms， 获取锁失败， 3100ms)
```

org.apache.rocketmq.broker.processor.PullMessageProcessor.processRequest(io.netty.channel.ChannelHandlerContext, org.apache.rocketmq.remoting.protocol.RemotingCommand)