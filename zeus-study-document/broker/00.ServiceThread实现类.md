ServiceThread
    - AcceptSocketService in HAService
    - AclFileWatchService
    - AllocateMappedFileService
    - CommitRealTimeService in CommitLog
    - FileWatchService
    - FlushCommitLogService in CommitLog
    - FlushConsumeQueueService in DefaultMessageStore
    - FlushDiskWatcher
    - FlushRealTimeService in CommitLog
    - GroupCommitService in CommitLog
    - GroupTransferService in HAService
    - HAClient in HAService
    - LmqPullRequestHoldService
    - PullMessageService - ok
    - PullRequestHoldService
    - ReadSocketService in HAConnection
    - RebalanceService - ok
    - ReputMessageService in DefaultMessageStore
    - StoreStatsService - ok
    - TransactionalMessageCheckService
    - WriteSocketService in HAConnection



## RebalanceService 消费端 - 消费队列负载均衡

遍历 MQClientInstance 的 ConcurrentMap<String, MQConsumerInner> consumerTable (当前客户端中维护的 消费组对应的客户端 DefaultMQPushConsumerImpl)

然后调用 RebalanceImpl 的 doRebalance 方法进行再平衡

```log
1. 获取当前维护消费端的订阅消息 (ConcurrentMap<String, SubscriptionData> subscriptionInner, Topic 名称和对应的订阅消息)
2. 进行遍历, 对每个 Topic 进行再平衡
3. 获取当前 Topic 的消息队列 (ConcurrentMap<String, Set<MessageQueue>> topicSubscribeInfoTable, 消息队列和对应的处理队列)
4. 获取当前 Topic 的所有消费者 Id (向 Broker 发送了 GET_CONSUMER_LIST_BY_GROUP = 38 请求码， cid 格式: IP@端口#1887994150767625， 暂时不知道是什么， 查看 org.apache.rocketmq.client.ClientConfig#buildMQClientId)
5. 对所有的队列和消费者 id 进行排序
6. 调用自身维护的分配策略进行分配 AllocateMessageQueueStrategy allocate 方法分配，当前消费端对应的消费队列 (入参有个 cid, 调用时传入当前消费端的消费者 id)
7. 遍历当前消费者维护的处理的队列关系 (ConcurrentMap<MessageQueue, ProcessQueue> processQueueTable, 消息队列和队列处理信息)
    1. 已有的队列关系中的队列在新分配到的队列列表中 (这个队列还是由当前消费者处理), 跳过这个队列
    2. 向 Broker 更新当前消费者的对应队列的消费进度 (向 Broker 发送了 UPDATE_CONSUMER_OFFSET = 15 请求码)
    3. 清除当前消费者的消费进度, 从 Map 中删除这个队列的信息 (RemoteBrokerOffsetStore 的 ConcurrentMap<MessageQueue, AtomicLong> offsetTable)
    4. Push 模式下且当前消费者是有序消费，且是集群消费，那么尝试从Broker端将该消息队列解锁，如果是并发消费，则不会解锁
8. 遍历分配到的队列 
    1. 对应的队列再重新分配前就有处理, 跳过这个队列
    2. 尝试将当前队列的消费进度从当前消费端进行删除 (RemoteBrokerOffsetStore 的 ConcurrentMap<MessageQueue, AtomicLong> offsetTable)
    3. 向 Broker 查询对应队列的消费进度 (向 Broker 发送了 QUERY_CONSUMER_OFFSET = 14 请求码)
    4. 将查询到的消费进度设置到 OffsetStore 中 (RemoteBrokerOffsetStore 的 ConcurrentMap<MessageQueue, AtomicLong> offsetTable)
    5. 向 前消费者处理的队列关系 (ConcurrentMap<MessageQueue, ProcessQueue> processQueueTable, 消息队列和队列处理信息) 添加当前的队列信息
    6. 向 拉取消息线程 PullMessageService 追加一个拉取消息的请求
9. 向 Broker 发送心跳信息 (向 Broker 发送了 HEART_BEAT = 34 请求码), 消息中会包含当前消费端的订阅信息
```

* Broker 在下面的场景
> 1 Broker 收到心跳请求之后如果发现消息中有新的 consumer 连接或者 consumer 订阅了新的 topic 或者移除了 topic 的订阅
> 2 如果某个客户端连接出现连接异常事件 EXCEPTION、连接断开事件 CLOSE、或者连接闲置事件 IDLE

向客户端发送请求码 NOTIFY_CONSUMER_IDS_CHANGED = 40, 通知客户端需要再平衡
客户端收到后, 触发 org.apache.rocketmq.client.impl.ClientRemotingProcessor#notifyConsumerIdsChanged 方法, 立即唤醒 RebalanceService 的线程, 进行再平衡

* 消费端启动时, 也会立即执行一次 RebalanceImpl 的 doRebalance, 通过再平衡, 分配消费队列
* RebalanceService 自身会 20s 唤醒一次, 进行再平衡

## PullMessageService 消费端 - 拉取消息

在再平衡中, 会向 PullMessageService 的队列 (LinkedBlockingQueue<PullRequest> pullRequestQueue) 添加一个拉取消息的请求 PullRequest
在 Push 模式下, 这些请求会被 PullMessageService 的线程消费

PullMessageService 的线程会从 pullRequestQueue 中取出 PullRequest, 然后执行 org.apache.rocketmq.client.impl.consumer.PullMessageService#pullMessage 方法

```log
1. 从 PullRequest 中获取对应的消费者组 (consumerGroup)
2. 从 MQClientInstance 的 ConcurrentMap<String, MQConsumerInner> consumerTable 中获取对应的消费者 DefaultMQPushConsumerImpl
3. 从 PullRequest 中获取对应的消息处理队列 (ProcessQueue)
4. ProcessQueue 的 dropped 为 true, 说明当前队列已经被丢弃, 跳过这个队列的拉取请求
5. 更新这个拉取请求的最后拉取时间 lastPullTimestamp 为当前的时间
6. 判断当前的消费者客户端的状态为 RUNNING, 否则跳过这个队列的拉取请求
7. 如果当前的消费者客户端的状态为 pause (再平衡中), 则跳过这个队列的拉取请求
8. 从 ProcessQueue 获取当前的缓存的消息的条数/消息大小, 如果超过了配置的最大值, 则在 50 毫秒后再将这个拉取请求添加到 PullMessageService 的队列中, 跳过这个队列的拉取请求
9. 如果当前的消费客户端是不是有序消费, 进行不同的属性赋值
    - 如果不是有序消费
    1. 从 ProcessQueue 的 TreeMap<Long, MessageExt> msgTreeMap 中获取第一个消息和最后一个消息的位点差, 如果超过了配置的位点差 (2000), 则在 50 毫秒后再将这个拉取请求添加到 PullMessageService 的队列中, 跳过这个队列的拉取请求

    - 如果是有序消费
    1. 从 ProcessQueue 的 locked 判断当前队列是否锁住了, 没有，则在 3000 毫秒后再将这个拉取请求添加到 PullMessageService 的队列中, 跳过这个队列的拉取请求
    2. 向 Broker 查询对应队列的消费进度 (向 Broker 发送了 QUERY_CONSUMER_OFFSET = 14 请求码)
    3. 更新 PullRequest 的 previouslyLocked 为 true (?)
    4. 更新 ProcessQueue 的下次请求到的位点 nextOffset 为从 Broker 请求到的消费进度
10. 从 RebalanceImpl 的 ConcurrentMap<String, SubscriptionData> subscriptionInner 获取 Topic 对应的订阅消息
11. 创建出一个回调函数 PullCallback, 在回调函数中处理拉取到的消息 (这个后面分析)
12. 如果是集群模式, 从 OffsetStore 中获取当前队列的消费进度, 如果这个值大于 0, 表示可以上报消费位点给 Broker, commitOffsetEnable 变为 true
13. 从 RebalanceImpl 的 ConcurrentMap<String, SubscriptionData> subscriptionInner 获取  Topic 对应的订阅消息
14. 调用 org.apache.rocketmq.client.impl.consumer.PullAPIWrapper#pullKernelImpl 方法进行拉取消息
15. 向 Broker 发送拉取消息的请求 (向 Broker 发送了 PULL_MESSAGE = 11 请求码), 发送的消息中有个 sysFlag 标示 (二进制的值，第二位标示请求是否可以挂起, 这里给的是 true)
16. 后面收到 Broker 发送的消息后, 会调用 PullCallback 的方法进行处理


// 收到消息后, 成功的处理逻辑
1. 更新 PullAPIWrapper 的 ConcurrentMap<MessageQueue, AtomicLong> pullFromWhichNodeTable (记录当前队列从哪个 Broker 拉取的消息)
2. 判断响应结果 PullResult 的状态
    - NO_NEW_MSG / NO_MATCHED_MSG(没有新消息 / 没有匹配的消息)
    1. 更新拉取请求 PullRequest 的下次请求位点 为 响应结果的 nextBeginOffset
    2. 对应的处理队列 ProcessQueue 的 msgCount 为 0 (当前队列没有消息), 更新 OffsetStore 的对应队列的消费进度为 nextBeginOffset
    3. 重新将这个拉取请求添加到 PullMessageService 的队列中

    - OFFSET_ILLEGAL 消费位点非法
    1. 更新拉取请求 PullRequest 的下次请求位点 为 响应结果的 nextBeginOffset
    2. 更新对应的处理队列 ProcessQueue 为丢弃状态 dropped = true
    3. 提交一个 10s 后执行的任务到线程池, 执行下面的逻辑
        3.1 更新设置到 OffsetStore 中 (RemoteBrokerOffsetStore 的 ConcurrentMap<MessageQueue, AtomicLong> offsetTable) 中对应队列的消费进度为 nextBeginOffset
        3.2 向 Broker 更新当前消费者的对应队列消费进度 (向 Broker 发送了 UPDATE_CONSUMER_OFFSET = 15 请求码)
        3.3 将这个队列和处理队列从 RebalanceImpl 的 ConcurrentMap<MessageQueue, ProcessQueue> processQueueTable 中删除
        3.4 向 Broker 更新当前消费者的对应队列消费进度 (向 Broker 发送了 UPDATE_CONSUMER_OFFSET = 15 请求码)
        3.5 清除当前消费者的消费进度, 从 Map 中删除这个队列的信息 (RemoteBrokerOffsetStore 的 ConcurrentMap<MessageQueue, AtomicLong> offsetTable)
        3.6 Push 模式下且当前消费者是有序消费，且是集群消费，那么尝试从Broker端将该消息队列解锁，如果是并发消费，则不会解锁

    - FOUND 找到消息
    1. 从 RebalanceImpl 的 ConcurrentMap<String, SubscriptionData> subscriptionInner 获取  Topic 对应的订阅消息
    2. 从 SubscriptionData 获取对应订阅的 Tags, 过滤掉获取到的消息的 Tags 不在需要的 Tags 中的消息
    3. 遍历剩余的消息的 TRAN_MSG 属性 (是否为事务消息), 如果为 true, 将消息的 UNIQ_KEY 属性设置到消息的 transactionId(事务 id) 属性
    4. 遍历剩余的消息的, 给消息 MIN_OFFSET 属性设置为当前消息的 minOffset 属性, 给消息 MAX_OFFSET 属性设置为当前消息的 maxOffset 属性, 设置消息的 BrokerName 属性为 MessageQueue 的 BrokerName 属性
    5. 更新拉取请求 PullRequest 的下次请求位点 为 响应结果的 nextBeginOffset
    6. 如果响应的消息为空, 重新将这个拉取请求添加到 PullMessageService 的队列中, 结束
    7. 将所有的消息保存到 处理队列 ProcessQueue 的 TreeMap<Long, MessageExt> msgTreeMap 中, key 为对应消息的消费位点
    8. 根据消息, 更新 ProcessQueue 的 msgCount 和 msgSize
    9. 获取最后一个消息的 MAX_OFFSET 属性, 获取到时, 这个值减去当前消息的消费位点， 如果大于 0, 赋值给 ProccessQueue 的 msgAccCnt (未知)
    10. 将消息提交给 org.apache.rocketmq.client.impl.consumer.ConsumeMessageService#submitConsumeRequest (并发消费/顺序消费)
    11. 重新将这个拉取请求添加到 PullMessageService 的队列中
    12. 根据配置的延迟间隔, 延迟提交这个拉取消息的 PullRequest 到 PullMessageService 的队列中 (默认 0, 立即拉取)


org.apache.rocketmq.client.impl.consumer.ConsumeMessageService#submitConsumeRequest 消费消息

并发消费 ConsumeMessageConcurrentlyService
1. 获取消费者每次批量消费时，最多消费多少条消息
2. 获取到的消息个数小于配置的最大消费消息数, 将消息 List<MessageExt> + 消息队列 MessageQueue + 处理队列 ProcessQueue 封装为 org.apache.rocketmq.client.impl.consumer.ConsumeMessageConcurrentlyService.ConsumeRequest, 提交到线程池中
3. 按照配置的最大消费消息数, 将消息 List<MessageExt> 拆分为多个 ConsumeRequest, 提交到线程池中

顺序消费 ConsumeMessageOrderlyService
1. 将消息 消息队列 MessageQueue + 处理队列 ProcessQueue 封装为 org.apache.rocketmq.client.impl.consumer.ConsumeMessageOrderlyService.ConsumeRequest, 提交到线程池中 (注 2 个不同的 ConsumeRequest， 里面消费逻辑不一样)


org.apache.rocketmq.client.impl.consumer.ConsumeMessageConcurrentlyService.ConsumeRequest 的 run 方法
1. 获取处理队列 ProcessQueue 的丢弃属性 dropped, 如果为 true, 跳过这个消息的消费
2. 遍历所有的消息, 如果消息有 RETRY_TOPIC 属性, 设置他们的 topic 为这个属性值
3. 遍历所有的消息, 如果当前消费端有设置命名空间, 将命名空间加到消息的 topic 上，将最新的值重新设置为消息的 topic
4. 当前的消费客户端有钩子函数，则执行钩子函数
5. 遍历所有的消息, 给消息的 CONSUME_START_TIME 属性设置为当前时间戳
6. 调用注册在 DefaultMQPushConsumerImpl 的 MessageListener 的 consumeMessage 方法进行消费, 获取执行结果
7. 当前的消费客户端有钩子函数，则执行钩子函数
8. 处理队列 ProcessQueue 的 的丢弃属性 dropped, 如果为 true, 跳过
9. 当前的消费端的消费模式为集群模式
    - 遍历所有消息, 向 Broker 更新消息消费响应 (向 Broker 发送了 CONSUMER_SEND_MSG_BACK = 36 请求码)
    - 向 Broker 更新失败的消息保存下来，延迟 5000ms 后, 重新将这批失败的消息 + 消息队列 MessageQueue + 处理队列 ProcessQueue 封装为 ConsumeRequest, 提交到线程池中
    - 获取最新的消费进度, 如果提交的消费位点大于 0, 同时处理队列未丢弃, 向 Broker 更新当前消费者的对应队列的消费进度 (向 Broker 发送了 UPDATE_CONSUMER_OFFSET = 15 请求码)




org.apache.rocketmq.client.impl.consumer.ConsumeMessageOrderlyService.ConsumeRequest 的 run 方法

1. 获取处理队列 ProcessQueue 的丢弃属性 dropped, 如果为 true, 跳过这个消息的消费
2. 获取这个队列的锁, 获取到才执行
3. 当前消费客户端为广播模式 || 处理队列 ProcessQueue 的锁住状态 locked 为 true 并且锁住没有超时
    - 符合条件
    1. 再次获取处理队列 ProcessQueue 的丢弃属性 dropped, 如果为 true, 跳过这个消息的消费
    2. 如果是集群模式同时 ProcessQueue 的锁住状态 locked 为 false，延迟多少毫秒后, 重新将消息队列 MessageQueue + 处理队列 ProcessQueue 封装为 ConsumeRequest, 提交到线程池中 (重新获取队列的锁成功， 延迟 110ms， 获取锁失败， 3100ms)
    3. 如果是集群模式同时 ProcessQueue 的锁住状态 locked 为 true, 但是锁超时了, 延迟多少毫秒后, 重新将消息队列 MessageQueue + 处理队列 ProcessQueue 封装为 ConsumeRequest, 提交到线程池中 (重新获取队列的锁成功， 延迟 110ms， 获取锁失败， 3100ms)
    4. 进入锁的时间超过了 60000ms, 延迟多少毫秒后, 重新将消息队列 MessageQueue + 处理队列 ProcessQueue 封装为 ConsumeRequest, 提交到线程池中 (重新获取队列的锁成功， 延迟 110ms， 获取锁失败， 3100ms)
    5. 消费者每次批量消费时，最多消费多少条消息, 从 ProcessQueue 的 TreeMap<Long, MessageExt> msgTreeMap 中获取对应条数的消息
    6. 遍历所有的消息, 如果消息有 RETRY_TOPIC 属性, 设置他们的 topic 为这个属性值
    7. 遍历所有的消息, 如果当前消费端有设置命名空间, 将命名空间加到消息的 topic 上，将最新的值重新设置为消息的 topic
    8. 当前的消费客户端有钩子函数，则执行钩子函数
    9. 获取消费队列 ProcessQueue 的消费锁 Lock consumeLock
    10. 调用注册在 DefaultMQPushConsumerImpl 的 MessageListener 的 consumeMessage 方法进行消费, 获取执行结果, 然后释放锁
    11. 如果消息是自动提交
        - 自动提交
        1. 执行结果为 暂时挂起
            1.1 判断所以需要处理的消息的重试次数是否超过了配置的最大重试次数 (默认为 Interger.MAX_VALUE), 超过了, 消息默认提交, 即执行完成 （更新处理队列的 ProcessQueue 的 msgSize 和 msgCount）
            1.2 没有超过, 延迟 3000ms 后, 重新将消息队列 MessageQueue + 处理队列 ProcessQueue 封装为 ConsumeRequest, 提交到线程池中
        2. 执行结果为 提交/回滚/消费成功
            2.1 消息提交, 即执行完成 （更新处理队列的 ProcessQueue 的 msgSize 和 msgCount）
        - 非自动提交
            1.1 状态为提交, 进行消息提交, 即执行完成 （更新处理队列的 ProcessQueue 的 msgSize 和 msgCount）
            1.2 状态为回滚, 重新将消费的消息提交到处理队列 ProcessQueue 的 TreeMap<Long, MessageExt> msgTreeMap 中, 延迟 3000ms 后, 重新将消息队列 MessageQueue + 处理队列 ProcessQueue 封装为 ConsumeRequest, 提交到线程池中
            1.3 转提为暂时挂起, 延迟 3000ms 后, 重新将消息队列 MessageQueue + 处理队列 ProcessQueue 封装为 ConsumeRequest, 提交到线程池中
    12. 如果提交的消费位点大于 0, 同时处理队列未丢弃, 向 Broker 更新当前消费者的对应队列的消费进度 (向 Broker 发送了 UPDATE_CONSUMER_OFFSET = 15 请求码)        

    - 不符合条件
    1. 再次获取处理队列 ProcessQueue 的丢弃属性 dropped, 如果为 true, 跳过这个消息的消费
    2. 延迟多少毫秒后, 重新将消息队列 MessageQueue + 处理队列 ProcessQueue 封装为 ConsumeRequest, 提交到线程池中 (重新获取队列的锁成功， 延迟 110ms， 获取锁失败， 3100ms)
```

org.apache.rocketmq.broker.processor.PullMessageProcessor.processRequest(io.netty.channel.ChannelHandlerContext, org.apache.rocketmq.remoting.protocol.RemotingCommand)

```log  
1. 如果当前的 Broker 的权限设置的是不可读, 返回权限不足的响应
2. 从 BrokerController 的 SubscriptionGroupManager subscriptionGroupManager 中获取对应消费者组的订阅信息 SubscriptionGroupConfig subscriptionGroupConfig (ConcurrentMap<String, SubscriptionGroupConfig> subscriptionGroupTable key: 消费者组名称)
3. 获取不到订阅信息, 返回没有订阅的响应
4. 获取订阅信息 SubscriptionGroupConfig 的是否可消费属性 consumeEnable, 如果为 false, 返回权限不足的响应
5. 从请求头中依次解析出 请求是否可以挂起 hasSuspendFlag, 请求是否有偏移量 hasCommitOffsetFlag, 请求是否有订阅表达式 hasSubscriptionFlag, 如果请求可以挂起, 获取挂起的最大时间 suspendTimeoutMillisLong
6. 从 BrokerController 的 TopicConfigManager topicConfigManager 中获取对应 Topic 的 Topic 信息 (ConcurrentMap<String, TopicConfig> topicConfigTable key:  Topic 名称)
7. 获取不到 Topic 信息, 返回 Topic 不存在的响应
8. 通过 Topic 配置信息，判断是否可读，不可读返回权限不足的响应
9. 请求中的队列 Id 小于 0 或者大于 Topic 配置信息中可读队列的 Id, 返回系统错误的响应
10. 获取订阅数据 SubscriptionData subscriptionData, 和 消费过滤数据 ConsumerFilterData consumerFilterData (订阅模式 不是 Tag, 而是 SQL 时必须有)
    - 如果请求有订阅表达式 hasSubscriptionFlag == true
    1. 从请求中解析出 subscriptionData 和 consumerFilterData
    - 如果请求没有订阅表达式 hasSubscriptionFlag == false (默认情况)
    1. 从 BrokerController 的 ConsumerManager consumerManager 中获取对消费组信息 ConsumerGroupInfo consumerGroupInfo (ConcurrentMap<String, ConsumerGroupInfo> consumerTable, key: 消费者组名称)
    2. 获取不到消费组信息, 返回订阅信息不存在的响应
    3. 如果订阅信息 subscriptionGroupConfig 不是启用广播模式, 但是消费组信息 ConsumerGroupInfo 是广播模式, 返回权限不足的响应
    4. 从消费组信息 ConsumerGroupInfo 中获取对应的订阅信息 SubscriptionData subscriptionData （ConcurrentMap<String, SubscriptionData> subscriptionTable， key: Topic 名称）
    5. 获取不到订阅信息, 返回订阅信息不存在的响应
    6. 请求中的子版本 (默认是时间戳) 大于订阅信息 subscriptionData 的子版本 (subVersion), 返回订阅信息不是最新的响应
    7. 如果订阅模式不是 Tag 模式, 从 BrokerController 的 ConsumerFilterManager consumerFilterManager 中获取对应的消费过滤数据 ConsumerFilterData consumerFilterData (ConcurrentMap<String, FilterDataMapByTopic> filterDataByTopic, key: Topic 名称, FilterDataMapByTopic 中有一个属性  ConcurrentMap<String, ConsumerFilterData> key: 消费者组名称), 获取到的 ConsumerFilterData 为 null, 返回过滤信息不存在, 对应的客户端版本小于请求的子版本 (subVersion), 返回过滤信息不是最新的响应
11. 如果订阅模式不是 Tag 模式， Broker 不支持非 Tag 模式的订阅 (enablePropertyFilter 为 false), 返回系统错误的响应
12. 根据 Broker 配置 filterSupportRetry （消息重试时, 是否支持按照条件进行过滤）, 创建不同的消息过滤器 MessageFilter messageFilter (true: ExpressionForRetryMessageFilter, false: ExpressionMessageFilter)
13. 从 BrokerController 的 DefaultMessageStore 中获取需要的消息
    - 获取不到消息
    1. 先将响应结果设置为系统错
    - 获取到了消息
    1. 获取请求到消息的响应结果
    2. 根据响应结果的是否建议从节点拉取 + 当前节点是否可读, 设置请求头的 suggestWhichBrokerId 为 0/1 (0 为主节点， 1 为从节点)
    3. 根据响应结果的 code, 设置到请求的响应 code
    4. 根据响应结果的结果
        - 请求成功: 根据 Broker 配置的 transferMsgByHeap (是否读取消息到堆内存中, 默认为 true) 是否为 true, 为 true 写入到响应体 (后面响应)，否则直接通过 Netty 推送给 consumer
        - 拉取为空
            4.1 根据请求参数的挂起时间 + Broker 是否支持长沦陷 + 配置的 shortPollingTimeMills (Broker 挂起时间, 默认 1s) 计算下次请求的挂起时间
            4.2 封装出一个拉取请求 PullRequest, 添加到 PullRequestHoldService 的 ConcurrentMap<String, ManyPullRequest> pullRequestTable, key:topic@queueId, value:ManyPullRequest (里面是一个 List, 存放了所有的 PullRequest), 后面有消息来时, 唤醒这个，发送消息
        - 拉取偏移量异常
            4.1 如果是不是从节点 或者 从节点允许 offset 校验 推送 offset 移出事件 org.apache.rocketmq.broker.processor.PullMessageProcessor#generateOffsetMovedEvent
            4.2 其他节点情况，设置响应 code 为 PULL_RETRY_IMMEDIATELY, consumer 收到响应后会立即从 MASTER 重试拉取

14. brokerAllowSuspend 和 hasCommitOffsetFlag + 当前节点不是从节点
    - 将请求中的 commitOffset 提交到 BrokerController 的 ConsumerOffsetManager consumerOffsetManager 的 ConcurrentMap<String, ConcurrentMap<Integer, Long>> offsetTable 中 （key1: topic@group, key2: queueId, value: offset）
15. 响应客户端    

获取消息 org.apache.rocketmq.store.DefaultMessageStore#getMessage

1. 判断 MessageStore 是否为关闭状态, 是否启动状态, 如果非正常状态, 返回 null
2. topic 是否以 %LMQ% 开头 + 当前 Broker 是否开启了 Lmq 功能, 如果非, 返回 null
3. 获取当前 commitLog 的最大偏移量
4. 通过 Topic 和 队列 Id 从 ConcurrentMap<String, ConcurrentMap<Integer, ConsumeQueue>> consumeQueueTable, 获取对应的 ConsumerQueue (不存在会，进行创建， ConsumeQueue 大小默认为 30w 条数据 * 20B, 所以每个 ConsumeQueue 的文件名位  6000000 的倍数)
5. 从 ConsumeQueue 获取最大和最小的偏移量, 如果请求的偏移量不在这个范围内, 计算下次开始请求的偏移量, 返回结果 (找不到消息的状态， 和开始重新请求的偏移量)
6. 根据请求的偏移量, 获取对应 ConsumeQueue 中 请求位置 + 当前文件可读位置间的缓存 SelectMappedBufferResult
7. 计算每次最小的过滤消息字节数, 默认为 16000/20B = 800 条消息， 如果单次拉取的消息数大于这个 800, 按照拉取的消息数进行计算
8. 按照每次 20B 的消息大小, 从 SelectMappedBufferResult 中获取消息, 上限为计算出来的最小的过滤消息字节数 或 SelectMappedBufferResult 的大小, 对每条消息进行判断
    8.1 依次获取消息的 0-7 个字节 (消息在 commitLog 的物理偏移量), 8-11 个字节 (消息的大小), 12-19 个字节 (tagsCode, 消息 tag 的 hash 值)
    8.2 计算根据消息的物理偏移量, 判断消息是在内存, 还是在 commitLog 文件中 (commitLog 最多的偏移量 - 消息的偏移量 > 当前 Broker 所在集群内存的 40%， 则默认为在 commitLog 文件中)
    8.3 如果消息在 commitLog 文件中, 从 commitLog 文件中获取消息, 如果消息在内存中, 从内存中获取消息
    8.4 判断是拉取的消息是否达到上限了 (涉及多个方面, 请求的条数到了， 从磁盘获取的消息最大 64k, 7 条， 从内存获取最大 256k, 31 条), 达上限, 跳出循环
    8.5 获取的 tagsCode <= Integer.MIN_VALUE, 表示这条消息有额外消息需要判断 (一般不会有)
    8.6 根据 tag 和消息的 tagsCode 进行判断, 是否符合过滤条件
    8.7 根据物理地址 + 消息大小, 从 commitLog 获取对应的消息
    8.8 根据消息的属性过滤 (SQL92 过滤, 需要根据消息的内容进行过滤)
    8.9 这条消息符合条件, 添加到结果集中
9. 返回查询结果 (结果集 + 查询状态 + 下次请求的偏移量 + 最大偏移量 + 最小偏移量)   
```

## PullRequestHoldService

```log
1. 进行挂起, 如果支持长轮训挂起时间 5s, 否则就是配置的短轮询时间 1s
2. 挂起唤醒, 遍历整个拉取请求 Map  ConcurrentMap<String, ManyPullRequest> pullRequestTable
3. 将 key 按照 @ 切割出 Topic 和 队列 Id, 从 ConsumeQueue 中获取最大的偏移量 
4. 遍历 ManyPullRequest 中所有的挂起请求 PullRequest
5. PullRequest 获取的偏移量还是大于 从 ConsumeQueue 中获取最大的偏移量, 再查询一次 ConsumeQueue 的最大偏移量
6. 计算出来的偏移量还是小于请求的偏移量
    6.1 当前挂起时间是否超过配置的时间, 提交一个响应客户端的任务到线程池
    6.2 没有超过, 再次挂起, 即将这个请求添加到 ConcurrentMap<String, ManyPullRequest> pullRequestTable
7. 计算出来的偏移量还是大于请求的偏移量    
    7.1 通过请求里面的消息过滤器，简单判断一遍消息是否符合条件
    7.2 符合条件, 重新提交一个拉取消息请求到线程池
    7.3 不符合条件
        - 当前挂起时间是否超过配置的时间, 提交一个响应客户端的任务到线程池
        - 没有超过, 再次挂起, 即将这个请求添加到 ConcurrentMap<String, ManyPullRequest> pullRequestTable 
```


## ReputMessageService

每隔 1ms 执行一次 org.apache.rocketmq.store.DefaultMessageStore.ReputMessageService.doReput
```log
1. 如果 ReputMessageService 维护的重放开始偏移量 reputFromOffset 小于 commitLog 的最小偏移量, 重放开始偏移量设置为 commitLog 的最小偏移量
2. 如果重放开始偏移量 大于等于 commitLog 的最大偏移量, 跳过这次循环
3. 如果消息允许重复复制 (默认为 false) 并且 reputFromOffset 大于等于已确定的偏移量 confirmOffset, 那么结束循环 (一遍都是 false)
4. 获取 重放开始偏移量 reputFromOffset 到 commitLog 的可读位置间的 Buffer
5. 将截取的 Buffer 的起始物理偏移量 赋值给 重放偏起始移量 reputFromOffset
6. 检查消息的属性并生成 DispatchRequest 对象 (封装了消息的 Topic, QueueId 等信息)
7. 获取消息的大小
8. 如果 DispatchRequest 的校验结果是否成功
    - 成功
        8.1 消息大小为 0, 表示读取到 MappedFile 文件尾, 将 重放偏起始移量 reputFromOffset 设置为下一个 commitLog 的开始偏移量
            8.1.1 如果这时读取到的大小还没达到 Buffer 的大小, 进行读取剩余的数据
        8.2 消息大小不为 0, 重放偏移量增加消息的大小, 循环调用 DefaultMessageStore 中的 LinkedList<CommitLogDispatcher> dispatcherList 的 dispatch 方法进行消息的分发
        8.3 开启了长轮询并且角色为主节点，则通知有新消息到达, 调用 NotifyMessageArrivingListener 的 arrive 方法, 通知消息到达
        8.4 需要消息中有 INNER_MULTI_DISPATCH/INNER_MULTI_QUEUE_OFFSET 2 个属性, 表示消息分发到多个队列 (正常不会), 对队列依次调用NotifyMessageArrivingListener 的 arrive 方法
        8.5 重放偏移量 reputFromOffset 增加消息的大小
    - 失败
        8.1 跳过这段消息, 重放偏移量增加消息的大小

NotifyMessageArrivingListener.arrive 方法
里面维护了一个 PullRequestHoldService, 立即调用里面的 notifyMessageArriving 方法, 通知消息到达 PullRequestHoldService 的线程, 唤醒执行的逻辑
```

LinkedList<CommitLogDispatcher> dispatcherList 2 个成员
CommitLogDispatcherBuildConsumeQueue
CommitLogDispatcherBuildIndex