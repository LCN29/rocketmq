ServiceThread
    - PullMessageService                                            ok
    - RebalanceService                                              ok

    - PullRequestHoldService                                        ok
    - ReputMessageService in DefaultMessageStore                    ok 
    - AllocateMappedFileService                                     ok
    - FileWatchService                                              ok
    - FlushDiskWatcher                                              ok

    - AcceptSocketService in HAService
    - GroupTransferService in HAService
    - HAClient in HAService
    - ReadSocketService in HAConnection
    - WriteSocketService in HAConnection

    - AclFileWatchService

    - CommitRealTimeService in CommitLog
    - FlushCommitLogService in CommitLog
    - FlushRealTimeService in CommitLog
    - GroupCommitService in CommitLog


    - FlushConsumeQueueService in DefaultMessageStore

    - LmqPullRequestHoldService
    - StoreStatsService                                          
    - TransactionalMessageCheckService




## RebalanceService 消费端 - 消费队列负载均衡

遍历 MQClientInstance 的 ConcurrentMap<String, MQConsumerInner> consumerTable (当前客户端中维护的 消费组对应的客户端 DefaultMQPushConsumerImpl)

然后调用 RebalanceImpl 的 doRebalance 方法进行再平衡

```log
1. 获取当前维护消费端的订阅消息 (ConcurrentMap<String, SubscriptionData> subscriptionInner, Topic 名称和对应的订阅消息)
2. 进行遍历, 对每个 Topic 进行再平衡
3. 获取当前 Topic 的消息队列 (ConcurrentMap<String, Set<MessageQueue>> topicSubscribeInfoTable, 消息队列和对应的处理队列)
4. 获取当前 Topic 的所有消费者 Id (向 Broker 发送了 GET_CONSUMER_LIST_BY_GROUP = 38 请求码， cid 格式: IP@端口#1887994150767625， 暂时不知道是什么， 查看 org.apache.rocketmq.client.ClientConfig#buildMQClientId)
5. 对所有的队列和消费者 id 进行排序
6. 调用自身维护的分配策略进行分配 AllocateMessageQueueStrategy allocate 方法分配，当前消费端对应的消费队列 (入参有个 cid, 调用时传入当前消费端的消费者 id)
7. 遍历当前消费者维护的处理的队列关系 (ConcurrentMap<MessageQueue, ProcessQueue> processQueueTable, 消息队列和队列处理信息)
    1. 已有的队列关系中的队列在新分配到的队列列表中 (这个队列还是由当前消费者处理), 跳过这个队列
    2. 向 Broker 更新当前消费者的对应队列的消费进度 (向 Broker 发送了 UPDATE_CONSUMER_OFFSET = 15 请求码)
    3. 清除当前消费者的消费进度, 从 Map 中删除这个队列的信息 (RemoteBrokerOffsetStore 的 ConcurrentMap<MessageQueue, AtomicLong> offsetTable)
    4. Push 模式下且当前消费者是有序消费，且是集群消费，那么尝试从Broker端将该消息队列解锁，如果是并发消费，则不会解锁
8. 遍历分配到的队列 
    1. 对应的队列再重新分配前就有处理, 跳过这个队列
    2. 尝试将当前队列的消费进度从当前消费端进行删除 (RemoteBrokerOffsetStore 的 ConcurrentMap<MessageQueue, AtomicLong> offsetTable)
    3. 向 Broker 查询对应队列的消费进度 (向 Broker 发送了 QUERY_CONSUMER_OFFSET = 14 请求码)
    4. 将查询到的消费进度设置到 OffsetStore 中 (RemoteBrokerOffsetStore 的 ConcurrentMap<MessageQueue, AtomicLong> offsetTable)
    5. 向 前消费者处理的队列关系 (ConcurrentMap<MessageQueue, ProcessQueue> processQueueTable, 消息队列和队列处理信息) 添加当前的队列信息
    6. 向 拉取消息线程 PullMessageService 追加一个拉取消息的请求
9. 向 Broker 发送心跳信息 (向 Broker 发送了 HEART_BEAT = 34 请求码), 消息中会包含当前消费端的订阅信息
```

* Broker 在下面的场景
> 1 Broker 收到心跳请求之后如果发现消息中有新的 consumer 连接或者 consumer 订阅了新的 topic 或者移除了 topic 的订阅
> 2 如果某个客户端连接出现连接异常事件 EXCEPTION、连接断开事件 CLOSE、或者连接闲置事件 IDLE

向客户端发送请求码 NOTIFY_CONSUMER_IDS_CHANGED = 40, 通知客户端需要再平衡
客户端收到后, 触发 org.apache.rocketmq.client.impl.ClientRemotingProcessor#notifyConsumerIdsChanged 方法, 立即唤醒 RebalanceService 的线程, 进行再平衡

* 消费端启动时, 也会立即执行一次 RebalanceImpl 的 doRebalance, 通过再平衡, 分配消费队列
* RebalanceService 自身会 20s 唤醒一次, 进行再平衡

## PullMessageService 消费端 - 拉取消息

在再平衡中, 会向 PullMessageService 的队列 (LinkedBlockingQueue<PullRequest> pullRequestQueue) 添加一个拉取消息的请求 PullRequest
在 Push 模式下, 这些请求会被 PullMessageService 的线程消费

PullMessageService 的线程会从 pullRequestQueue 中取出 PullRequest, 然后执行 org.apache.rocketmq.client.impl.consumer.PullMessageService#pullMessage 方法

```log
1. 从 PullRequest 中获取对应的消费者组 (consumerGroup)
2. 从 MQClientInstance 的 ConcurrentMap<String, MQConsumerInner> consumerTable 中获取对应的消费者 DefaultMQPushConsumerImpl
3. 从 PullRequest 中获取对应的消息处理队列 (ProcessQueue)
4. ProcessQueue 的 dropped 为 true, 说明当前队列已经被丢弃, 跳过这个队列的拉取请求
5. 更新这个拉取请求的最后拉取时间 lastPullTimestamp 为当前的时间
6. 判断当前的消费者客户端的状态为 RUNNING, 否则跳过这个队列的拉取请求
7. 如果当前的消费者客户端的状态为 pause (再平衡中), 则跳过这个队列的拉取请求
8. 从 ProcessQueue 获取当前的缓存的消息的条数/消息大小, 如果超过了配置的最大值, 则在 50 毫秒后再将这个拉取请求添加到 PullMessageService 的队列中, 跳过这个队列的拉取请求
9. 如果当前的消费客户端是不是有序消费, 进行不同的属性赋值
    - 如果不是有序消费
    1. 从 ProcessQueue 的 TreeMap<Long, MessageExt> msgTreeMap 中获取第一个消息和最后一个消息的位点差, 如果超过了配置的位点差 (2000), 则在 50 毫秒后再将这个拉取请求添加到 PullMessageService 的队列中, 跳过这个队列的拉取请求

    - 如果是有序消费
    1. 从 ProcessQueue 的 locked 判断当前队列是否锁住了, 没有，则在 3000 毫秒后再将这个拉取请求添加到 PullMessageService 的队列中, 跳过这个队列的拉取请求
    2. 向 Broker 查询对应队列的消费进度 (向 Broker 发送了 QUERY_CONSUMER_OFFSET = 14 请求码)
    3. 更新 PullRequest 的 previouslyLocked 为 true (?)
    4. 更新 ProcessQueue 的下次请求到的位点 nextOffset 为从 Broker 请求到的消费进度
10. 从 RebalanceImpl 的 ConcurrentMap<String, SubscriptionData> subscriptionInner 获取 Topic 对应的订阅消息
11. 创建出一个回调函数 PullCallback, 在回调函数中处理拉取到的消息 (这个后面分析)
12. 如果是集群模式, 从 OffsetStore 中获取当前队列的消费进度, 如果这个值大于 0, 表示可以上报消费位点给 Broker, commitOffsetEnable 变为 true
13. 从 RebalanceImpl 的 ConcurrentMap<String, SubscriptionData> subscriptionInner 获取  Topic 对应的订阅消息
14. 调用 org.apache.rocketmq.client.impl.consumer.PullAPIWrapper#pullKernelImpl 方法进行拉取消息
15. 向 Broker 发送拉取消息的请求 (向 Broker 发送了 PULL_MESSAGE = 11 请求码), 发送的消息中有个 sysFlag 标示 (二进制的值，第二位标示请求是否可以挂起, 这里给的是 true)
16. 后面收到 Broker 发送的消息后, 会调用 PullCallback 的方法进行处理


// 收到消息后, 成功的处理逻辑
1. 更新 PullAPIWrapper 的 ConcurrentMap<MessageQueue, AtomicLong> pullFromWhichNodeTable (记录当前队列从哪个 Broker 拉取的消息)
2. 判断响应结果 PullResult 的状态
    - NO_NEW_MSG / NO_MATCHED_MSG(没有新消息 / 没有匹配的消息)
    1. 更新拉取请求 PullRequest 的下次请求位点 为 响应结果的 nextBeginOffset
    2. 对应的处理队列 ProcessQueue 的 msgCount 为 0 (当前队列没有消息), 更新 OffsetStore 的对应队列的消费进度为 nextBeginOffset
    3. 重新将这个拉取请求添加到 PullMessageService 的队列中

    - OFFSET_ILLEGAL 消费位点非法
    1. 更新拉取请求 PullRequest 的下次请求位点 为 响应结果的 nextBeginOffset
    2. 更新对应的处理队列 ProcessQueue 为丢弃状态 dropped = true
    3. 提交一个 10s 后执行的任务到线程池, 执行下面的逻辑
        3.1 更新设置到 OffsetStore 中 (RemoteBrokerOffsetStore 的 ConcurrentMap<MessageQueue, AtomicLong> offsetTable) 中对应队列的消费进度为 nextBeginOffset
        3.2 向 Broker 更新当前消费者的对应队列消费进度 (向 Broker 发送了 UPDATE_CONSUMER_OFFSET = 15 请求码)
        3.3 将这个队列和处理队列从 RebalanceImpl 的 ConcurrentMap<MessageQueue, ProcessQueue> processQueueTable 中删除
        3.4 向 Broker 更新当前消费者的对应队列消费进度 (向 Broker 发送了 UPDATE_CONSUMER_OFFSET = 15 请求码)
        3.5 清除当前消费者的消费进度, 从 Map 中删除这个队列的信息 (RemoteBrokerOffsetStore 的 ConcurrentMap<MessageQueue, AtomicLong> offsetTable)
        3.6 Push 模式下且当前消费者是有序消费，且是集群消费，那么尝试从Broker端将该消息队列解锁，如果是并发消费，则不会解锁

    - FOUND 找到消息
    1. 从 RebalanceImpl 的 ConcurrentMap<String, SubscriptionData> subscriptionInner 获取  Topic 对应的订阅消息
    2. 从 SubscriptionData 获取对应订阅的 Tags, 过滤掉获取到的消息的 Tags 不在需要的 Tags 中的消息
    3. 遍历剩余的消息的 TRAN_MSG 属性 (是否为事务消息), 如果为 true, 将消息的 UNIQ_KEY 属性设置到消息的 transactionId(事务 id) 属性
    4. 遍历剩余的消息的, 给消息 MIN_OFFSET 属性设置为当前消息的 minOffset 属性, 给消息 MAX_OFFSET 属性设置为当前消息的 maxOffset 属性, 设置消息的 BrokerName 属性为 MessageQueue 的 BrokerName 属性
    5. 更新拉取请求 PullRequest 的下次请求位点 为 响应结果的 nextBeginOffset
    6. 如果响应的消息为空, 重新将这个拉取请求添加到 PullMessageService 的队列中, 结束
    7. 将所有的消息保存到 处理队列 ProcessQueue 的 TreeMap<Long, MessageExt> msgTreeMap 中, key 为对应消息的消费位点
    8. 根据消息, 更新 ProcessQueue 的 msgCount 和 msgSize
    9. 获取最后一个消息的 MAX_OFFSET 属性, 获取到时, 这个值减去当前消息的消费位点， 如果大于 0, 赋值给 ProccessQueue 的 msgAccCnt (未知)
    10. 将消息提交给 org.apache.rocketmq.client.impl.consumer.ConsumeMessageService#submitConsumeRequest (并发消费/顺序消费)
    11. 重新将这个拉取请求添加到 PullMessageService 的队列中
    12. 根据配置的延迟间隔, 延迟提交这个拉取消息的 PullRequest 到 PullMessageService 的队列中 (默认 0, 立即拉取)


org.apache.rocketmq.client.impl.consumer.ConsumeMessageService#submitConsumeRequest 消费消息

并发消费 ConsumeMessageConcurrentlyService
1. 获取消费者每次批量消费时，最多消费多少条消息
2. 获取到的消息个数小于配置的最大消费消息数, 将消息 List<MessageExt> + 消息队列 MessageQueue + 处理队列 ProcessQueue 封装为 org.apache.rocketmq.client.impl.consumer.ConsumeMessageConcurrentlyService.ConsumeRequest, 提交到线程池中
3. 按照配置的最大消费消息数, 将消息 List<MessageExt> 拆分为多个 ConsumeRequest, 提交到线程池中

顺序消费 ConsumeMessageOrderlyService
1. 将消息 消息队列 MessageQueue + 处理队列 ProcessQueue 封装为 org.apache.rocketmq.client.impl.consumer.ConsumeMessageOrderlyService.ConsumeRequest, 提交到线程池中 (注 2 个不同的 ConsumeRequest， 里面消费逻辑不一样)


org.apache.rocketmq.client.impl.consumer.ConsumeMessageConcurrentlyService.ConsumeRequest 的 run 方法
1. 获取处理队列 ProcessQueue 的丢弃属性 dropped, 如果为 true, 跳过这个消息的消费
2. 遍历所有的消息, 如果消息有 RETRY_TOPIC 属性, 设置他们的 topic 为这个属性值
3. 遍历所有的消息, 如果当前消费端有设置命名空间, 将命名空间加到消息的 topic 上，将最新的值重新设置为消息的 topic
4. 当前的消费客户端有钩子函数，则执行钩子函数
5. 遍历所有的消息, 给消息的 CONSUME_START_TIME 属性设置为当前时间戳
6. 调用注册在 DefaultMQPushConsumerImpl 的 MessageListener 的 consumeMessage 方法进行消费, 获取执行结果
7. 当前的消费客户端有钩子函数，则执行钩子函数
8. 处理队列 ProcessQueue 的 的丢弃属性 dropped, 如果为 true, 跳过
9. 当前的消费端的消费模式为集群模式
    - 遍历所有消息, 向 Broker 更新消息消费响应 (向 Broker 发送了 CONSUMER_SEND_MSG_BACK = 36 请求码)
    - 向 Broker 更新失败的消息保存下来，延迟 5000ms 后, 重新将这批失败的消息 + 消息队列 MessageQueue + 处理队列 ProcessQueue 封装为 ConsumeRequest, 提交到线程池中
    - 获取最新的消费进度, 如果提交的消费位点大于 0, 同时处理队列未丢弃, 向 Broker 更新当前消费者的对应队列的消费进度 (向 Broker 发送了 UPDATE_CONSUMER_OFFSET = 15 请求码)




org.apache.rocketmq.client.impl.consumer.ConsumeMessageOrderlyService.ConsumeRequest 的 run 方法

1. 获取处理队列 ProcessQueue 的丢弃属性 dropped, 如果为 true, 跳过这个消息的消费
2. 获取这个队列的锁, 获取到才执行
3. 当前消费客户端为广播模式 || 处理队列 ProcessQueue 的锁住状态 locked 为 true 并且锁住没有超时
    - 符合条件
    1. 再次获取处理队列 ProcessQueue 的丢弃属性 dropped, 如果为 true, 跳过这个消息的消费
    2. 如果是集群模式同时 ProcessQueue 的锁住状态 locked 为 false，延迟多少毫秒后, 重新将消息队列 MessageQueue + 处理队列 ProcessQueue 封装为 ConsumeRequest, 提交到线程池中 (重新获取队列的锁成功， 延迟 110ms， 获取锁失败， 3100ms)
    3. 如果是集群模式同时 ProcessQueue 的锁住状态 locked 为 true, 但是锁超时了, 延迟多少毫秒后, 重新将消息队列 MessageQueue + 处理队列 ProcessQueue 封装为 ConsumeRequest, 提交到线程池中 (重新获取队列的锁成功， 延迟 110ms， 获取锁失败， 3100ms)
    4. 进入锁的时间超过了 60000ms, 延迟多少毫秒后, 重新将消息队列 MessageQueue + 处理队列 ProcessQueue 封装为 ConsumeRequest, 提交到线程池中 (重新获取队列的锁成功， 延迟 110ms， 获取锁失败， 3100ms)
    5. 消费者每次批量消费时，最多消费多少条消息, 从 ProcessQueue 的 TreeMap<Long, MessageExt> msgTreeMap 中获取对应条数的消息
    6. 遍历所有的消息, 如果消息有 RETRY_TOPIC 属性, 设置他们的 topic 为这个属性值
    7. 遍历所有的消息, 如果当前消费端有设置命名空间, 将命名空间加到消息的 topic 上，将最新的值重新设置为消息的 topic
    8. 当前的消费客户端有钩子函数，则执行钩子函数
    9. 获取消费队列 ProcessQueue 的消费锁 Lock consumeLock
    10. 调用注册在 DefaultMQPushConsumerImpl 的 MessageListener 的 consumeMessage 方法进行消费, 获取执行结果, 然后释放锁
    11. 如果消息是自动提交
        - 自动提交
        1. 执行结果为 暂时挂起
            1.1 判断所以需要处理的消息的重试次数是否超过了配置的最大重试次数 (默认为 Interger.MAX_VALUE), 超过了, 消息默认提交, 即执行完成 （更新处理队列的 ProcessQueue 的 msgSize 和 msgCount）
            1.2 没有超过, 延迟 3000ms 后, 重新将消息队列 MessageQueue + 处理队列 ProcessQueue 封装为 ConsumeRequest, 提交到线程池中
        2. 执行结果为 提交/回滚/消费成功
            2.1 消息提交, 即执行完成 （更新处理队列的 ProcessQueue 的 msgSize 和 msgCount）
        - 非自动提交
            1.1 状态为提交, 进行消息提交, 即执行完成 （更新处理队列的 ProcessQueue 的 msgSize 和 msgCount）
            1.2 状态为回滚, 重新将消费的消息提交到处理队列 ProcessQueue 的 TreeMap<Long, MessageExt> msgTreeMap 中, 延迟 3000ms 后, 重新将消息队列 MessageQueue + 处理队列 ProcessQueue 封装为 ConsumeRequest, 提交到线程池中
            1.3 转提为暂时挂起, 延迟 3000ms 后, 重新将消息队列 MessageQueue + 处理队列 ProcessQueue 封装为 ConsumeRequest, 提交到线程池中
    12. 如果提交的消费位点大于 0, 同时处理队列未丢弃, 向 Broker 更新当前消费者的对应队列的消费进度 (向 Broker 发送了 UPDATE_CONSUMER_OFFSET = 15 请求码)        

    - 不符合条件
    1. 再次获取处理队列 ProcessQueue 的丢弃属性 dropped, 如果为 true, 跳过这个消息的消费
    2. 延迟多少毫秒后, 重新将消息队列 MessageQueue + 处理队列 ProcessQueue 封装为 ConsumeRequest, 提交到线程池中 (重新获取队列的锁成功， 延迟 110ms， 获取锁失败， 3100ms)
```

org.apache.rocketmq.broker.processor.PullMessageProcessor.processRequest(io.netty.channel.ChannelHandlerContext, org.apache.rocketmq.remoting.protocol.RemotingCommand)

```log  
1. 如果当前的 Broker 的权限设置的是不可读, 返回权限不足的响应
2. 从 BrokerController 的 SubscriptionGroupManager subscriptionGroupManager 中获取对应消费者组的订阅信息 SubscriptionGroupConfig subscriptionGroupConfig (ConcurrentMap<String, SubscriptionGroupConfig> subscriptionGroupTable key: 消费者组名称)
3. 获取不到订阅信息, 返回没有订阅的响应
4. 获取订阅信息 SubscriptionGroupConfig 的是否可消费属性 consumeEnable, 如果为 false, 返回权限不足的响应
5. 从请求头中依次解析出 请求是否可以挂起 hasSuspendFlag, 请求是否有偏移量 hasCommitOffsetFlag, 请求是否有订阅表达式 hasSubscriptionFlag, 如果请求可以挂起, 获取挂起的最大时间 suspendTimeoutMillisLong
6. 从 BrokerController 的 TopicConfigManager topicConfigManager 中获取对应 Topic 的 Topic 信息 (ConcurrentMap<String, TopicConfig> topicConfigTable key:  Topic 名称)
7. 获取不到 Topic 信息, 返回 Topic 不存在的响应
8. 通过 Topic 配置信息，判断是否可读，不可读返回权限不足的响应
9. 请求中的队列 Id 小于 0 或者大于 Topic 配置信息中可读队列的 Id, 返回系统错误的响应
10. 获取订阅数据 SubscriptionData subscriptionData, 和 消费过滤数据 ConsumerFilterData consumerFilterData (订阅模式 不是 Tag, 而是 SQL 时必须有)
    - 如果请求有订阅表达式 hasSubscriptionFlag == true
    1. 从请求中解析出 subscriptionData 和 consumerFilterData
    - 如果请求没有订阅表达式 hasSubscriptionFlag == false (默认情况)
    1. 从 BrokerController 的 ConsumerManager consumerManager 中获取对消费组信息 ConsumerGroupInfo consumerGroupInfo (ConcurrentMap<String, ConsumerGroupInfo> consumerTable, key: 消费者组名称)
    2. 获取不到消费组信息, 返回订阅信息不存在的响应
    3. 如果订阅信息 subscriptionGroupConfig 不是启用广播模式, 但是消费组信息 ConsumerGroupInfo 是广播模式, 返回权限不足的响应
    4. 从消费组信息 ConsumerGroupInfo 中获取对应的订阅信息 SubscriptionData subscriptionData （ConcurrentMap<String, SubscriptionData> subscriptionTable， key: Topic 名称）
    5. 获取不到订阅信息, 返回订阅信息不存在的响应
    6. 请求中的子版本 (默认是时间戳) 大于订阅信息 subscriptionData 的子版本 (subVersion), 返回订阅信息不是最新的响应
    7. 如果订阅模式不是 Tag 模式, 从 BrokerController 的 ConsumerFilterManager consumerFilterManager 中获取对应的消费过滤数据 ConsumerFilterData consumerFilterData (ConcurrentMap<String, FilterDataMapByTopic> filterDataByTopic, key: Topic 名称, FilterDataMapByTopic 中有一个属性  ConcurrentMap<String, ConsumerFilterData> key: 消费者组名称), 获取到的 ConsumerFilterData 为 null, 返回过滤信息不存在, 对应的客户端版本小于请求的子版本 (subVersion), 返回过滤信息不是最新的响应
11. 如果订阅模式不是 Tag 模式， Broker 不支持非 Tag 模式的订阅 (enablePropertyFilter 为 false), 返回系统错误的响应
12. 根据 Broker 配置 filterSupportRetry （消息重试时, 是否支持按照条件进行过滤）, 创建不同的消息过滤器 MessageFilter messageFilter (true: ExpressionForRetryMessageFilter, false: ExpressionMessageFilter)
13. 从 BrokerController 的 DefaultMessageStore 中获取需要的消息
    - 获取不到消息
    1. 先将响应结果设置为系统错
    - 获取到了消息
    1. 获取请求到消息的响应结果
    2. 根据响应结果的是否建议从节点拉取 + 当前节点是否可读, 设置请求头的 suggestWhichBrokerId 为 0/1 (0 为主节点， 1 为从节点)
    3. 根据响应结果的 code, 设置到请求的响应 code
    4. 根据响应结果的结果
        - 请求成功: 根据 Broker 配置的 transferMsgByHeap (是否读取消息到堆内存中, 默认为 true) 是否为 true, 为 true 写入到响应体 (后面响应)，否则直接通过 Netty 推送给 consumer
        - 拉取为空
            4.1 根据请求参数的挂起时间 + Broker 是否支持长沦陷 + 配置的 shortPollingTimeMills (Broker 挂起时间, 默认 1s) 计算下次请求的挂起时间
            4.2 封装出一个拉取请求 PullRequest, 添加到 PullRequestHoldService 的 ConcurrentMap<String, ManyPullRequest> pullRequestTable, key:topic@queueId, value:ManyPullRequest (里面是一个 List, 存放了所有的 PullRequest), 后面有消息来时, 唤醒这个，发送消息
        - 拉取偏移量异常
            4.1 如果是不是从节点 或者 从节点允许 offset 校验 推送 offset 移出事件 org.apache.rocketmq.broker.processor.PullMessageProcessor#generateOffsetMovedEvent
            4.2 其他节点情况，设置响应 code 为 PULL_RETRY_IMMEDIATELY, consumer 收到响应后会立即从 MASTER 重试拉取

14. brokerAllowSuspend 和 hasCommitOffsetFlag + 当前节点不是从节点
    - 将请求中的 commitOffset 提交到 BrokerController 的 ConsumerOffsetManager consumerOffsetManager 的 ConcurrentMap<String, ConcurrentMap<Integer, Long>> offsetTable 中 （key1: topic@group, key2: queueId, value: offset）
15. 响应客户端    

获取消息 org.apache.rocketmq.store.DefaultMessageStore#getMessage

1. 判断 MessageStore 是否为关闭状态, 是否启动状态, 如果非正常状态, 返回 null
2. topic 是否以 %LMQ% 开头 + 当前 Broker 是否开启了 Lmq 功能, 如果非, 返回 null
3. 获取当前 commitLog 的最大偏移量
4. 通过 Topic 和 队列 Id 从 ConcurrentMap<String, ConcurrentMap<Integer, ConsumeQueue>> consumeQueueTable, 获取对应的 ConsumerQueue (不存在会，进行创建， ConsumeQueue 大小默认为 30w 条数据 * 20B, 所以每个 ConsumeQueue 的文件名位  6000000 的倍数)
5. 从 ConsumeQueue 获取最大和最小的偏移量, 如果请求的偏移量不在这个范围内, 计算下次开始请求的偏移量, 返回结果 (找不到消息的状态， 和开始重新请求的偏移量)
6. 根据请求的偏移量, 获取对应 ConsumeQueue 中 请求位置 + 当前文件可读位置间的缓存 SelectMappedBufferResult
7. 计算每次最小的过滤消息字节数, 默认为 16000/20B = 800 条消息， 如果单次拉取的消息数大于这个 800, 按照拉取的消息数进行计算
8. 按照每次 20B 的消息大小, 从 SelectMappedBufferResult 中获取消息, 上限为计算出来的最小的过滤消息字节数 或 SelectMappedBufferResult 的大小, 对每条消息进行判断
    8.1 依次获取消息的 0-7 个字节 (消息在 commitLog 的物理偏移量), 8-11 个字节 (消息的大小), 12-19 个字节 (tagsCode, 消息 tag 的 hash 值)
    8.2 计算根据消息的物理偏移量, 判断消息是在内存, 还是在 commitLog 文件中 (commitLog 最多的偏移量 - 消息的偏移量 > 当前 Broker 所在集群内存的 40%， 则默认为在 commitLog 文件中)
    8.3 如果消息在 commitLog 文件中, 从 commitLog 文件中获取消息, 如果消息在内存中, 从内存中获取消息
    8.4 判断是拉取的消息是否达到上限了 (涉及多个方面, 请求的条数到了， 从磁盘获取的消息最大 64k, 7 条， 从内存获取最大 256k, 31 条), 达上限, 跳出循环
    8.5 获取的 tagsCode <= Integer.MIN_VALUE, 表示这条消息有额外消息需要判断 (一般不会有)
    8.6 根据 tag 和消息的 tagsCode 进行判断, 是否符合过滤条件
    8.7 根据物理地址 + 消息大小, 从 commitLog 获取对应的消息
    8.8 根据消息的属性过滤 (SQL92 过滤, 需要根据消息的内容进行过滤)
    8.9 这条消息符合条件, 添加到结果集中
9. 返回查询结果 (结果集 + 查询状态 + 下次请求的偏移量 + 最大偏移量 + 最小偏移量)   
```

## PullRequestHoldService

```log
1. 进行挂起, 如果支持长轮训挂起时间 5s, 否则就是配置的短轮询时间 1s
2. 挂起唤醒, 遍历整个拉取请求 Map  ConcurrentMap<String, ManyPullRequest> pullRequestTable
3. 将 key 按照 @ 切割出 Topic 和 队列 Id, 从 ConsumeQueue 中获取最大的偏移量 
4. 遍历 ManyPullRequest 中所有的挂起请求 PullRequest
5. PullRequest 获取的偏移量还是大于 从 ConsumeQueue 中获取最大的偏移量, 再查询一次 ConsumeQueue 的最大偏移量
6. 计算出来的偏移量还是小于请求的偏移量
    6.1 当前挂起时间是否超过配置的时间, 提交一个响应客户端的任务到线程池
    6.2 没有超过, 再次挂起, 即将这个请求添加到 ConcurrentMap<String, ManyPullRequest> pullRequestTable
7. 计算出来的偏移量还是大于请求的偏移量    
    7.1 通过请求里面的消息过滤器，简单判断一遍消息是否符合条件
    7.2 符合条件, 重新提交一个拉取消息请求到线程池
    7.3 不符合条件
        - 当前挂起时间是否超过配置的时间, 提交一个响应客户端的任务到线程池
        - 没有超过, 再次挂起, 即将这个请求添加到 ConcurrentMap<String, ManyPullRequest> pullRequestTable 
```


## ReputMessageService

每隔 1ms 执行一次 org.apache.rocketmq.store.DefaultMessageStore.ReputMessageService.doReput
```log
1. 如果 ReputMessageService 维护的重放开始偏移量 reputFromOffset 小于 commitLog 的最小偏移量, 重放开始偏移量设置为 commitLog 的最小偏移量
2. 如果重放开始偏移量 大于等于 commitLog 的最大偏移量, 跳过这次循环
3. 如果消息允许重复复制 (默认为 false) 并且 reputFromOffset 大于等于已确定的偏移量 confirmOffset, 那么结束循环 (一遍都是 false)
4. 获取 重放开始偏移量 reputFromOffset 到 commitLog 的可读位置间的 Buffer
5. 将截取的 Buffer 的起始物理偏移量 赋值给 重放偏起始移量 reputFromOffset
6. 检查消息的属性并生成 DispatchRequest 对象 (封装了消息的 Topic, QueueId 等信息)
7. 获取消息的大小
8. 如果 DispatchRequest 的校验结果是否成功
    - 成功
        8.1 消息大小为 0, 表示读取到 MappedFile 文件尾, 将 重放偏起始移量 reputFromOffset 设置为下一个 commitLog 的开始偏移量
            8.1.1 如果这时读取到的大小还没达到 Buffer 的大小, 进行读取剩余的数据
        8.2 消息大小不为 0, 重放偏移量增加消息的大小, 循环调用 DefaultMessageStore 中的 LinkedList<CommitLogDispatcher> dispatcherList 的 dispatch 方法进行消息的分发
        8.3 开启了长轮询并且角色为主节点，则通知有新消息到达, 调用 NotifyMessageArrivingListener 的 arrive 方法, 通知消息到达
        8.4 需要消息中有 INNER_MULTI_DISPATCH/INNER_MULTI_QUEUE_OFFSET 2 个属性, 表示消息分发到多个队列 (正常不会), 对队列依次调用NotifyMessageArrivingListener 的 arrive 方法
        8.5 重放偏移量 reputFromOffset 增加消息的大小
    - 失败
        8.1 跳过这段消息, 重放偏移量增加消息的大小

NotifyMessageArrivingListener.arrive 方法
里面维护了一个 PullRequestHoldService, 立即调用里面的 notifyMessageArriving 方法, 通知消息到达 PullRequestHoldService 的线程, 唤醒执行的逻辑
```

LinkedList<CommitLogDispatcher> dispatcherList 3 个成员
CommitLogDispatcherCalcBitMap (在 BrokerController initialize 追加的)
CommitLogDispatcherBuildConsumeQueue
CommitLogDispatcherBuildIndex

```log
CommitLogDispatcherCalcBitMap: 根据 DispatchRequest 构建布隆过滤器，加速 SQL92 过滤效率，避免每次都解析sql

1. 是否启动计算过滤布隆过滤器 (默认 false)
2. https://blog.csdn.net/jjhfen00/article/details/132176491


CommitLogDispatcherBuildConsumeQueue: 根据 DispatchRequest 写 ConsumeQueue 文件，构建 ConsumeQueue 索引
1. 从 DispatchRequest 获取消息的事务类型
2. 事务准备中 / 事务回滚, 则不处理
3. 不是事务消息, 事务提交, 继续
4. 通过 topic + queueId 找到对应的 ConsumeQueue
5. 判断当前的文件存储是否可以写入, 不能跳过
6. 最多重试 30 次, 执行写入过程
7. 从 DispatchRequest 获取消息的 TagsCode 
8. 如果支持扩展信息写入 (默认 false), 通过 ConsumeQueueExt consumeQueueExt 将 TagsCode 做一个转换
9. 写消息到 ConsumerQueue 文件中
    9.1 消息偏移量+消息大小 小于等于 ConsumeQueue 已处理的最大物理偏移量, 消息已经处理了, 直接返回
    9.2 Buffer 限制 20B, 依次写入消息偏移量 + 消息大小 + 消息的 tagsCode
    9.3 已存在索引数据的最大预计偏移量 + 20B 得到新的偏移量, 通过这个偏移量获取对应的 MappedFile 
    9.4 获取不到对应的 MappedFile, 直接返回
    9.5 如果 mappedFile 是第一个创建的消费队列，并且消息在消费队列的偏移量不为 0，并且消费队列写入指针为 0, 表示索引文件有异常, 进行重设索引信息, 更新最新提交位置 + 文件最新偏移量 + 最新偏移量填充 0 
    9.6 消息在消费队列偏移量为 0, 表示此前没有数据, 直接将 Buffer 写入文件, 更新最新的物理偏移量
    9.7 消息在消费队列偏移量不为 0, 表示此前有数据, 
        9.7.1 获取当前 MappedFIle 的逻辑偏移量, 当前的偏移量大于最新的文件偏移量, 表示重复构建消费队列, 直接返回
        9.7.2 直接将 Buffer 写入文件, 更新最新的物理偏移量
10. 写入失败, 沉睡 100s, 然后重试
11. 写入成功, 走下面的流程
    11.1 如果是从节点/开启了 DLeger 功能, 更新 check point 的物理存储时间 physicMsgTimestamp 为 commitLog 文件的刷盘时间戳, 单位毫秒 (主节点, 再写入 commitLog, 实际已经保存一遍了)
    11.2 更新 check point 的逻辑存储时间 logicsMsgTimestamp 为 commitLog 文件的刷盘时间戳 (Consume Queue 存储时间)
    11.3 如果是分发队列, 更新各个对象的信息


CommitLogDispatcherBuildIndex: 根据 DispatchRequest 写 IndexFile 文件, 构建 IndexFile 索引
1. 判断是否支持 IndexFile, 默认 true, 不支持, 返回
2. 获取或创建索引文件 IndexFile
    2.1 IndexService 的 ArrayList<IndexFile> indexFileList 不为空, 获取最后一个, 如果最后一个没有写满, 返回这个
    2.2 创建一个新的 IndexService, 添加到 ArrayList<IndexFile> indexFileList
    2.3 创建一个线程, 将上一个 indexFile (原 indexFileList 最后一个) 刷入磁盘 (过程更新 check point 的 indexMsgTimestamp)
    2.4 返回创建的 IndexFile
3. 如果消息在 commitLog 中的偏移量小于该文件的结束索引在 commitLog 中的偏移量, 那么表示已为该消息之后的消息构建 Index 索引, 返回
4. 如果是事务回滚消息，则直接返回，不需要创建索引     
5. 获取消息的唯一 id (msgId), 如果有, 将其添加到 IndexFile 中
    5.1 以 topic#消息 id 构建为唯一存入的内容
    5.2 将 key 转为正数的 hashCode 
    5.3 通过 hashCode & hash 槽数量(默认 500w) 的方式获取当前 key 对应的 hash 槽下标位置
    5.4 计算该消息的绝对 hash 槽偏移量 absSlotPos = 40 + slotPos (上一步计算到的 hash 位置) * 4
    5.5 获取上一步计算的绝对位置在 hash 槽的值，一个 hash 槽大小为 4B
    5.6 如果值不为 0 说明这个 hash key 已经存在，即存在 hash 冲突
    5.7 计算当前消息在 commitLog 中的消息存储时间与该 Index 文件起始时间差
    5.8 获取该消息的索引存放位置的绝对偏移量 absIndexPos = 40B + 500w * 4B + indexCount * 20B
    5.9 将 hashCode(absIndexPos), 消息在 commit Log 的偏移量 (absIndexPos + 4), 消息在 commitLog 中的消息存储时间与该 Index 文件起始时间差 (absIndexPos + 4 + 8),  hash 槽的值 (absIndexPos + 4 + 8 + 4), 最新的 IndexFile 的索引条目计数的编号 (abs)
    5.10 更新 IndexFile 的头部 索引个数 + 最后的物理偏移量 + 结束的时间戳
    5.11 这个过程, 如果添加失败, 或尝试获取下一个新的 IndexFile
6. 获取消息的 keys, 按照空格切割为多个 key, 将 key 按照唯一 id 存入 IndexFile
```

## AllocateMappedFileService

org.apache.rocketmq.store.AllocateMappedFileService#putRequestAndReturnMappedFile
```log 
1. 创建一个分配下一个文件的请求 AllocateRequest, 添加到 ConcurrentMap<String, AllocateRequest> requestTable 和 PriorityBlockingQueue<AllocateRequest> requestQueue
2. 再创建一个分配下下一个文件的请求 AllocateRequest, 添加到 ConcurrentMap<String, AllocateRequest> requestTable 和 PriorityBlockingQueue<AllocateRequest> requestQueue
3. requestTable 的 key 是文件名, map 无序, 所以添加一份到 Queue, 达到按顺序分配
4. 尝试通过 下一个文件的文件名从 map 中获取 分配请求, 获取不到返回 null
5. 获取到了, 线程阻塞在这个请求的 countDown 5s, 等待唤醒
6. 唤醒成功后，将这个请求从 requestTable 移除, 返回分配请求里面的 mappedFile 
```

org.apache.rocketmq.store.AllocateMappedFileService#mmapOperation
```log 
1. 从 PriorityBlockingQueue<AllocateRequest> requestQueue 获取一个分配请求 (并移除这个请求)
2. 从 ConcurrentMap<String, AllocateRequest> requestTable 获取这个请求, 获取不到, 直接返回 true, 唤醒阻塞的线程
3. 判断队列里面的请求 和 从 Map 中获取的请求是同一个, 不是返回 true, 唤醒阻塞的线程
4. 分配请求里面的 mappedFile 是否为 null, 不为 null, 唤醒阻塞的线程
5. 如果开启了 TransientStorePool, 通过池化方式创建 MappedFile, 池化创建异常/没有开启池化, 通过 new MappedFile 创建 MappedFile 
6. 创建的 mappedFile 的文件大小 大于等于 commitLog 默认的 1G + 开启了文件的预热功能 (默认 false), 对 mappedFile 进行预热 
7. 将创建的 mappedFile 设置到分配请求的 mappedFile 属性中, 直接返回 true, 唤醒阻塞的线程

```

## FlushRealTimeService
异步刷盘 (没有开启堆外线程池), 消息写入 commitLog 的 Buffer 后, 唤醒这个线程, 然后返回 PUT_OK

```log 
1. 获取是否是定时刷盘配置 (默认是 false)
2. 获取 刷新数据到磁盘的间隔 500ms
3. 获取刷盘的最少页数，默认 4, 即 16k
4. 获取 2 次刷盘最长延迟间隔时间，默认 10s, 即距离上一次刷盘超过 10s 时，不管页数是否超过 4, 都会刷盘
5. 上次刷盘时间 + 2 次刷盘最长延迟间隔时间 小于等于当前时间 (2 次刷盘时间间隔超过配置了), 设置本次刷盘最少页数为 0 (有就刷盘), 更新上次刷盘时间为当前时间
6. 如果是定时刷盘, sleep 刷新数据到磁盘的间隔 时间, 时间到了自然唤醒, 其间不会被唤醒, 非定时刷盘, 通过 AQS, 自旋 刷新数据到磁盘的间隔 时间, 可以被其他线程唤醒
7. 调用 MappedFileQueue mappedFileQueue 的 flush 方法进行刷盘
8. 调用 MappedFileQueue mappedFileQueue 的 getStoreTimestamp 获取存储时间
9. 将存储时间 存储时间 更新到 checkPoint 的 commitLog 最新消息的写入时间 (physicMsgTimestamp)
```

## CommitRealTimeService
异步刷盘 (开启堆外线程池), 消息写入 commitLog 的 Buffer 后, 唤醒这个线程, 然后返回 PUT_OK

提交: 将堆外缓存 Buffer 的数据写入到 FileChannel 中, 在通过 FlushRealTimeService 将 FileChannel 中的数据刷盘

```log 
1. 获取提交间隔时间 (获取刷新数据到 FileChannel 的时间间隔， ), 默认 200ms 
2. 获取提交的最少页数 (每次刷新数据到 FileChannel 的页数), 默认 4, 即 16k
3. 获取 2 次提交最长延迟间隔时间，默认 10s, 即距离上一次刷盘超过 10s 时，不管页数是否超过 4, 都会刷盘
4. 上次提交时间 + 2 次刷盘最长延迟间隔时间 小于等于当前时间 (2 次提交时间间隔超过配置了), 设置本次提交最少页数为 0 (有就提交), 更新上次提交时间为当前时间
5. 调用 MappedFileQueue mappedFileQueue 的 commit 方法进行数据提交到 FileChannel
6. 如果提交成功, 更新上次提交时间 = 当前时间, 唤醒 FlushRealTimeService, 进行刷盘 
7. 进入等待 提交间隔时间 时间 (可被唤醒)
```



## GroupCommitService

同步刷盘
    - 不需要等待刷盘结果 - , 唤醒这个线程, 然后返回 PUT_OK
    - 需要等待刷盘结果 
        1. 封装出一个刷盘提交请求, 存放到 FlushDiskWatcher flushDiskWatcher 的 LinkedBlockingQueue<GroupCommitRequest> commitRequests (超时 5s)
        2. 将封装存储的刷盘提交请求，再存放一份到 GroupCommitService 的 LinkedList<GroupCommitRequest> requestsWrite
        3. 然后返回一个 CompletableFuture<PutMessageStatus> 用来给上游获取刷盘结果

```log 
1. 将内部中存放的 GroupCommitRequest 的 requestsWrite 和 requestsRead 互换
2. 如果 requestsRead 为空, 直接调用 MappedFileQueue mappedFileQueue 的 flush, 有多少刷多少
3. 如果 requestsRead 不为空, 遍历整个列表

```
        
## FileWatchService
监听文件变更, 然后执行 Listener 里面的逻辑
在 Broker 和 Nameserver 中有使用到，用于判断 ttl 文件的变更

## FlushDiskWatcher
监听 CommitLog  刷盘超时监控, 一个线程会不到从 LinkedBlockingQueue<GroupCommitRequest> commitRequests 获取任务
循环判断循环判断任务是否完成, 超时了直接设置任务失败