
## 发送消息类型

消息刷盘
    - 异步刷盘
        - 异步+关闭写缓冲
        - 异步+开启写缓冲
    - 同步刷盘
        - 不等待落盘 ack
        - 等待落盘 ack


FileChannel.write(ByteBuffer src);
FileChannel.force(boolean metaData);

MappedByteBuffer.put(ByteBuffer src);å
FileChannel.write(ByteBuffer src);
MappedByteBuffer.force()



消息
1. 写入 CommitLog
2. 更新 CheckPoint
3. 同步从节点
4. 写入 consumeQueue
5. 更新 CheckPoint
6. 写入 indexFile (如果有 key)
7. 更新 CheckPoint
8. 响应客户端的发送消息请求


ReputMessageService
消息写入 ConsumeQueue 和 indexFile 





NettyServer 处理的线程池 NettyServerCodecThread
|
V
RequestTask requestTask

processorTable 中注册的 code 对应的线程池 (value) (SendMessageThread_)

--> submit() 提交任务




SendMessageThread_
|
|
V
RemotingResponseCallback callback
从 processorTable 获取对应的处理器 NettyRequestProcessor(AsyncNettyRequestProcessor)

进入真正处理过程 asyncProcessRequest()

1. parseRequestHeader 获取请求头
2. asyncSendMessage 处理消息
- 检查对应的 topic， 队列是否存在
- 提前封装出 RemotingCommand response 响应结果
- 拼接出 MessageStore 需要的 MessageExtBrokerInner 对象
- 调用 MessageStore 的 asyncPutMessage 方法
    - checkStoreStatus() 检查存储状态 (broker 准备关闭, 从节点无法保存消息， 当前磁盘不可写了, PageCache 繁忙)
    - checkMessage() 检查消息是否合法 (topic 长度， 自定义属性字符串长度)
    - CommitLog.asyncPutMessage 消息写入
        - 给 MessageExtBrokerInner 设置当前的时间戳， crc 值
        - 获取 MessageStore 的 StoreStatsService storeStatsService 统计线程
        - 从 ThreadLocal 中获取 PutMessageThreadLocal, 把 MessageExtBrokerInner 的内容转为 转为 ByteBuf 放到里面的 MessageExtEncoder.ByteBuf 中
        - 根据消息 MessageExtBrokerInner 产生一个 Topic 名称 - 队列序号的 字符串, 作为入参，创建出 PutMessageContext 对象
        - 加上自旋锁/可重入锁 (由 MessageStore 的配置决定)
        - 从自身属性的 mappedFileQueue 中获取最新的一个 MappedFile 文件 (mappedFileQueue 中有一个分配线程 AllocateMappedFileService 负责分配)
        - 将自身属性的 beginTimeInLock 设置为当前时间 (用来其他线程判断，当前的 Page Cache 是否繁忙, > 0 说明繁忙)
        - 获取到的 MappedFile 为空或者已经满了(写指针的位置等于文件大小了), 在继续通过 mappedFileQueue 获取一个最新的 MappedFile
        - 调用 MappedFile 的 appendMessage 方法，将消息写入到 MappedFile 中
        - 写入成功后, 更新 beginTimeInLock 为 0, 释放锁
        - 调用 storeStatsService 的 方法, 更新统计信息
        - 根据配置的刷盘方式, 获取不同的 FlushCommitLogService 实现类，将消息刷盘, 返回一个 CompletableFuture 对象
        - 根据配置的主从配置, 决定不同的消息同步方式, 返回一个 CompletableFuture 对象
        - 封装出一个返回结果对象 PutMessageResult(先默认添加成功, MappedFile 写入的结果)
        - 封装出一个新的 CompletableFuture<PutMessageStatus>, 里面的逻辑: 等待 2 个 CompletableFuture 对象完成, 将结果设置到 PutMessageResult 的 putMessageStatus 中 (刷盘失败, 主从同步失败， 处理成功)
        - 返回 CompletableFuture<PutMessageStatus> 对象
    - 返回 CommitLog 的 CompletableFuture<PutMessageStatus> 对象
- 获取 MessageStore 的 asyncPutMessage 方法返回的 CompletableFuture<PutMessageResult>
- 追加新的逻辑
    - 给响应结果 RemotingCommand response 设置结果
    - 获取 Broker 的统计管理对象 BrokerStatsManager, 更新统计信息
    - 设置请求头的信息 responseHeader
    - 返回响应对象
- 获取 putMessageFutureExecutor 线程池, 执行 RemotingResponseCallback 的 callback 方法
    - 判断请求是否需要响应结果
    - 需要调用 ChannelHandlerContext 的 writeAndFlush 将 response 发生给客户端
  
## FlushCommitLogService 实现

CommitRealTimeService (异步刷盘 + 开启写缓冲)
    - 定时刷盘, 默认 500 毫秒 
    - CommitLog.MappedFileQueue 的 commit (FileChannel.write() 方法将数据写入 page cache)
    - 后续的操作由 FlushRealTimeService 进行操作, 进行落盘

FlushRealTimeService (异步刷盘 + 关闭写缓冲)
    - 定时刷盘, 默认 500 毫秒 
    - CommitLog.MappedFileQueue 的 flush 
    - CommitLog.DefaultMessageStore.StoreCheckpoint 更新其 physicMsgTimestamp


GroupCommitService
    （消息之前已经被 MappedByteBuffer写入了pageCache 了，这里主需要处理一下落盘）
    - 添加一个任务到内部的写列表中, 阻塞
    - 内部线程, 每 10ms 唤醒一次(可被其他线程中断)
    - 加锁, 读写队列交互
    - 循环从读队列中获取任务
    - CommitLog.MappedFileQueue 的 flush 
    - 设置结果到任务中
    - CommitLog.DefaultMessageStore.StoreCheckpoint 更新其 physicMsgTimestamp
    - 清空读队列


Message.isWaitStoreMsgOK() 是否需要等待消息落盘，进行响应
不等待落盘 ack, 写线程，唤醒 GroupCommitService 结束


疑问：为什么RocketMQ不依赖操作系统的异步刷盘，而费劲周章的设计如此刷盘策略呢？

个人理解，主要是考虑以下2点：
1、依赖操作系统异步刷盘，操作系统可能会在page cache上积累了大量数据后才会触发一次flush动作，这就可能会造成曲线上的毛刺现象，所以自己把控刷盘节奏，可有效避免毛刺现象的发生
2、作为一个成熟开源的组件，数据的安全性至关重要，还是要尽可能保证数据稳步有序落盘；OS的异步刷盘固然好使，但RocketMQ对其把控较弱，当操作系统crash或者断电的时候，造成的数据丢失影响不可控